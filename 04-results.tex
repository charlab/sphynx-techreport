\section{Results}

% expected results
There are two methods for analyzing the potential gains from shared
instruction caches.
The first is to keep the total instruction cache size across the chip
constant and increase the capacity each thread sees by grouping that
storage together.
This method of sharing will provide no area savings, but should grant
an increased hit rate for the now larger cache.
The increase will be limited by the working set of the application.
We project improvement in cache performance for this approach in
Figure~\ref{HitImprov}. 

\begin{figure}[b]
\centering
\includegraphics[width=\columnwidth]{graphics/HitRateImprov.png}
\caption{Projected instruction cache hit rate with fixed total cache size and increased sharing}
\label{HitImprov}
\end{figure}

The second method is to reduce the total storage required for
instruction cache data arrays while allowing at most a minor
degredation in hit rate.
We consider an approach where the instruction cache each processor
sees remains fixed but the number of threads sharing each cache is
increased. 
We expect that the hit rate of the cache may drop when the instruction
cache is not large enough to satisfy the demands from all the cores
sharing the cache.
However, the performance penalty should not be as dramatic as the
reduction in area when all threads run the same application.
Figure \ref{AreaEff} shows our rough expectations for this approach
which should improve the efficiency of the chip.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{graphics/AreaEff.png}
\caption{Projected cache area and hit rate due to increased sharing}
\label{AreaEff}
\end{figure}

\subsection{Setup}

We use GPGPU-Sim to analyze the effectiveness of our design because of
its ability to simulate a large number of parallel processing cores.
GPGPU-Sim~\cite{bakhodayuan09} is an open-source software package
available to simulate GPU architecture. 
It has been validated to be representative of performance on NVIDIA
GPUs and provides a reasonable platform for testing alternate
highly-parallel computer architectures, and provides a reference
configuration for a NVIDIA GTX580 GPU, which we used for our study.
The GTX580 contains 16 streaming multiprocessors with the NVIDIA Fermi
architecture. 
In GPGPU-Sim, each CUDA streaming multiprocessor is represented as a
single SIMT core. 
Each streaming multiprocressor can have up to 48 warps, with 32
threads per warp (See Figure \ref{GTX580}). 
All sixteen SIMT cores share unified L2 cache.

\jbs{What is the icache configuration?}


\begin{figure}[b]
\centering
\includegraphics[width=\columnwidth]{graphics/GTX580.jpg}
\caption{Block Diagram of NVIDIA GTX580~\cite{gf100}}
\label{GTX580}
\end{figure}

In the Fermi architecture, each streaming multiprocessor has
its own distinct L1 cache. 
Each L1 instruction cache is 4-way set associative, with 4 sets of
128 bytes per block. 
To assess the  performance of different L1 instruction cache
architectures, only the L1 instruction cache was modified, while all
other architectural variables remained constant. 
Other aspects of the architecture such as the bandwidth of shared L2
cache could serve as a performance bottleneck for different cache
designs. 
However, these variables are ignored for this study, because we expect
them to have little impact on the hit and stall rate of the
instruction cache, which are the primary metrics of interest.

\subsection{Benchmarks}

We used benchmarks provided by the standard GPGPU-Sim distribution,
including STO, RAY, NQU, LPS, LIB, CP and BFS.
In order to determine which of the aforementioned benchmarks would be
most useful for the purposes of our project, we performed an
experiment in which the performance of the benchmarks was examined as
a function of the number of cores used in the available GTX580
architecture. 
The number of cores was varied between 1 and 16 (the maximum number of
cores available on the GTX580). 
Then the miss and stall rate was noted for each different
architectural configuration. 
Figure \ref{fig:missStalls} shows the results of our experiment. 

\begin{figure}[b!]
\centering
\includegraphics[width=\columnwidth]{graphics/miss_stalls_benchmarks.png}
\caption{Miss and Stall Rates for Benchmark Programs with the GTX580
\jbs{Are the numbers (1-4) the same here as for
  Figure~\ref{HitImprov}? Why is this miss rate when the others are
hit rate? We should unify the presentation.}
}
\label{fig:missStalls}
\end{figure}

\begin{figure}[b!]
\centering
\includegraphics[width=\columnwidth]{graphics/miss_stalls_benchmarks_zoomed.png}
\caption{Miss and Stall Rates for Benchmark Programs with GTX580 (Zoomed In) }
\label{fig:missStallsZoomed}
\end{figure}

Figure \ref{fig:missStallsZoomed} is a zoomed in version of
\ref{fig:missStalls} included to enable better understanding of the
results with lower miss rates. 

The benchmark STO is mostly unaffected by this variation in the number of cores and RAY has a much higher
stall rate (indicating more reservation fails than misses) than its
miss rates, and is evidently more susceptible to the aforementioned change. 

RAY had more variability in how it was affected by the number of cores.

In order to explain the reasons behind the results shown in Figures
\ref{fig:missStalls} and \ref{fig:missStallsZoomed}, we examined the
degree of parallelism of RAY and CP.

The degree of parallelism is a representation of the number of software
work elements that each program issues to the GPU when executing in
hardware. 
Calculations on the number of issued work elements is a representation of the level of parallelism employed by
2 programs which we found interesting as per our previous results.

\emph{CP (Coulombic Potential)}

Given a standard configuration, CP issues a high number of thread blocks -- a representation of the high degree of parallelism that CP takes advantage of.

The reason only the parallelism of CP and RAY were examined was that
from a preliminary inspection, they seemed to having a desired
combination of parallelism and size of instruction set. 

From all the aforementioned results, it was clear that RAY was the
most interesting bechmark. 
This was because it had the highest number of PTX instructions issued
and was also significatly parallel. 
Moreover, the fact that the degree of parallelism of RAY could be
increased easily by modifying the CUDA program, we nominated RAY as
our most significant benchmark program.
 
Increasing the degree of parallelism in software entailed changing the
image size that the Ray Tracer would operate on.
 
By increasing the size of the picture to 512 by 512, we made RAY issue a much higher number of thread blocks per unit time. 

This ensured that the RAY benchmark was executing as parallel in
software as the Coulombic Potential benchmark in the number of work
elements that it was issuing. 
Given that the RAY CUDA file issues more instructions in its core
computation that CP does, and that a lot of its instructions (CUDA
instructions) seem to be computationally more expensive, RAY can now
be treated as both a long program which would occupy a significant
amount of space in the instruction cache (and hence would not be
entirely resident in the instruction cache) AND would issue those
instructions with a high degree of parallelism.

\subsection{Preliminary Results}
\jbs{Fabiha in charge}

The performed simulations point to several factors that may account 
for the outcomes seen with each approach. Parameters that are expected 
to affect instruction cache performance include the number of cores 
shared, associativity, parallelism, and cache size.

The first method, varying the number of cores sharing a cache while 
keeping the total cache size constant, was expected to improve the hit 
rate. As the number of cores sharing the cache increased, the miss rate 
and stall rate both increased, with the later increasing more rapidly 
due to the higher number of reservation fails. While the hit rate may 
have improved, there was an overall trend of increased reservation fails 
because the cache was preoccupied with instructions from the greater 
number of cores. As an exception to this, CP demonstrated a balance as 
the stall rate decreased going from 8 cores to 16 cores sharing the 
cache. This was most likely due to the higher level of parallelism that 
CP had in comparison to the other benchmarks. Meanwhile, smaller programs 
remained unaffected more often than larger benchmarks.

The second approach entailed reducing the total cache size by keeping 
the size of each individual instruction cache constant when merging the 
cores that share each one. The reduction in area of the now shared cache 
allowed duplicate instructions to be accessed more quickly. This 
affected benchmarks with many reused instructions and is expected to 
provide space to be used for other caches.

Overall, these methods appear to perform best on multi-threaded 
applications with many reused instructions. Meanwhile, for benchmarks 
with higher hit rates to begin with, and thus less reservation fails, 
this method made less of an impact.
